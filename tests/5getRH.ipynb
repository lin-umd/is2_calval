{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196e0be7-15d0-40f6-8cbe-888952fe70e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# read is2_calval\n",
    "import geopandas as gpd \n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "# read H5 file \n",
    "import h5py\n",
    "########\n",
    "import utm\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4211bd43-be16-4653-b2ba-44773432b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_is2 = gpd.read_parquet('../result/is2_20m_calval_09252023.parquet') # 18,550,387 IS2 20 m segment points\n",
    "# Specify the path to your GeoPackage file\n",
    "als_sites = \"../data/sites_20221006.gpkg\"\n",
    "# Read the GeoPackage file\n",
    "gdf_als = gpd.read_file(als_sites)\n",
    "# Now, gdf is a GeoDataFrame containing the data from the GeoPackage file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828fe23-6576-442a-9e15-14f85f769804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "45b83451-bdc5-4c90-8d10-c2515c959701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# now processing als project:  usa_fernan_tepeepottercreek\n",
      "# is2 points in this als project:  285\n",
      "# convert segments in utm...\n",
      "# number of laz files:  24\n",
      "# parallel processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                         | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of is2 20m segments in las file: # number of is2 20m segments in las file:   148\n",
      "\n",
      "# number of is2 20m segments in las file:  37\n",
      "# number of is2 20m segments in las file:  35\n",
      "# number of is2 20m segments in las file:  25\n",
      "# number of is2 20m segments in las file:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██▊              | 4/24 [00:00<00:00, 29.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of is2 20m segments in las file:  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████▉            | 7/24 [00:01<00:04,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of is2 20m segments in las file:  15\n",
      "# number of is2 20m segments in las file:  13\n",
      "# number of is2 20m segments in las file:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████▍          | 9/24 [00:05<00:11,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of is2 20m segments in las file:  8\n",
      "# number of is2 20m segments in las file:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████▎        | 11/24 [00:06<00:10,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of is2 20m segments in las file:  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████        | 12/24 [00:08<00:10,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of is2 20m segments in las file:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████▎      | 14/24 [00:23<00:17,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "##### get project name \n",
    "for index, row in gdf_als[3:4].iterrows(): \n",
    "        # Open a text file named \"output.txt\" in write mode\n",
    "        # with open('processing.txt', 'w') as file:\n",
    "        #     # Write text to the file\n",
    "        #     file.write('# index: {}\\n'.format(index))\n",
    "        #     file.write('# now processing als project: {}\\n'.format(row['region'] + '_' + row['name']))\n",
    "        # The file will be automatically closed when you exit the 'with' block\n",
    "        # the bounding box of each input geometry intersects the bounding box\n",
    "        als_index = gdf_is2.sindex.query(row['geometry']) # super fast!!!!! # but only boundary box. \n",
    "        is2_in_als = gdf_is2.loc[als_index] \n",
    "        #print(len(is2_in_als))\n",
    "        is2_in_als = is2_in_als.clip(row['geometry'])  # get points inside polygon.\n",
    "        #out_name = '../result/is2_calval_region/is2_' + row['region'] + '_' + row['name'] + '.parquet'\n",
    "        print('# now processing als project: ', row['region'] + '_' + row['name'])\n",
    "        #is2_in_als.to_parquet(out_name)   # save is2 footprints in this als site\n",
    "        print(\"# is2 points in this als project: \", len(is2_in_als))\n",
    "        print('# convert segments in utm...')\n",
    "        ##############################################################\n",
    "        for is2_index, is2_row in is2_in_als.iterrows():   \n",
    "                lat_c = is2_row['land_segments/latitude_20m'] \n",
    "                lon_c = is2_row['land_segments/longitude_20m']\n",
    "                e, n , zone, letter = utm.from_latlon(lat_c, lon_c)\n",
    "                is2_in_als.loc[is2_index, 'e'] = e\n",
    "                is2_in_als.loc[is2_index, 'n'] = n\n",
    "                is2_in_als.loc[is2_index, 'zone'] = zone\n",
    "        # Create a GeoDataFrame with a Point geometry\n",
    "        geometry = [Point(x, y) for x, y in zip(is2_in_als['e'], is2_in_als['n'])]\n",
    "        is2_in_als_utm = gpd.GeoDataFrame(data = is2_in_als, geometry=geometry)\n",
    "        ##################################################################################\n",
    "        data_path = '/gpfs/data1/vclgp/data/gedi/imported/' + row['region'] + '/' + row['name'] + '/LAZ_ground'\n",
    "        # convert all laz file to las files\n",
    "        files_path = data_path + '/*.laz' \n",
    "        laz_files = glob.glob(files_path)\n",
    "        print('# number of laz files: ', len(laz_files))\n",
    "        nCPU = len(laz_files)\n",
    "        if nCPU > 5 : \n",
    "           nCPU = 5  # number of cores to use  \n",
    "        print('# parallel processing...')\n",
    "        pool = multiprocessing.Pool(nCPU) # Set up multi-processing\n",
    "        progress_bar = tqdm(total=len(laz_files))\n",
    "        def update_progress_bar(_):\n",
    "              progress_bar.update()  \n",
    "        for laz_f in laz_files:\n",
    "            pool.apply_async(get_sim_laz, (is2_in_als_utm, laz_f), callback=update_progress_bar)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # Close the progress bar\n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bf6822a1-b04f-4230-ba63-fa1ac4239a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1 segnment ----29 footprints ---find id in the waveform .\n",
    "# give a laz, create a 20m buffer zone, return all laz in the zone. \n",
    "# return all is2 points in laz boundary \n",
    "def get_sim_laz(is2_in_als_utm , laz_path):\n",
    "#laz_path = '/gpfs/data1/vclgp/data/gedi/imported/usa/usda_or/LAZ_ground/or_10_101.laz'\n",
    "                    als_name = laz_path.split('/')[-3]\n",
    "                    region = laz_path.split('/')[-4]\n",
    "                    bounds_file = '/gpfs/data1/vclgp/data/gedi/imported/lists/ground_bounds/boundGround.' + als_name+ '.txt'\n",
    "                    df = pd.read_csv(bounds_file, header = None, sep = \" \")\n",
    "                    #print('# las file : \\n', laz_path)\n",
    "                    df.columns = ['file', 'xmin', 'ymin', 'zmin', 'xmax', 'ymax', 'zmax']\n",
    "                    basename = os.path.basename(laz_path)\n",
    "                    d_file = df[df['file'].str.contains(basename[:-4])]\n",
    "                    #print( '# laz file bounds: ',d_file.iloc[:, 1:6])\n",
    "                    xmin = d_file['xmin']\n",
    "                    xmax = d_file['xmax'] \n",
    "                    ymin = d_file['ymin'] \n",
    "                    ymax = d_file['ymax'] \n",
    "                    # get laz and nearby laz \n",
    "                    from shapely.geometry import Polygon\n",
    "                    # Create a Polygon geometry from the coordinates\n",
    "                    polygon = Polygon([(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)])\n",
    "                    # return is2_in_als -----\n",
    "                    is2_laz = is2_in_als_utm.clip(polygon)\n",
    "                    #print('# is2_laz: \\n', is2_laz)\n",
    "                    if (len(is2_laz) == 0):\n",
    "                        #print('# no is2 20m segment is in this las file!')\n",
    "                        return None\n",
    "                    # get footprints \n",
    "                    ###########################################################\n",
    "    ###########################################################################\n",
    "                    # should i get 1 las file  ---> 1 wave.h5?\n",
    "                    # start from one segment -- one row \n",
    "                    print('# number of is2 20m segments in las file: ', len(is2_laz)) \n",
    "                    ## ########read simulation waveform file \n",
    "                    out_wave = '../wave/wave_'+ basename[:-4] + '.h5'\n",
    "                    #print('# waveform file: ', out_wave)\n",
    "                    f_wave = h5py.File(out_wave, 'r')\n",
    "                    byte_strings = f_wave['WAVEID'][()]\n",
    "                    wave_ids = []\n",
    "                    for item in byte_strings:\n",
    "                        # Convert each byte string to a regular string and join them\n",
    "                        result_string = ''.join([byte.decode('utf-8') for byte in item])\n",
    "                        wave_ids.append(result_string)\n",
    "                    # Find common strings\n",
    "                    df1 = pd.DataFrame(wave_ids)\n",
    "                    df1.columns = ['id']\n",
    "                    df1['wave_index'] = df1.index\n",
    "                    #print(\"Keys in HDF5 file:\", list(f_wave.keys()))\n",
    "                    ########## loop every segment \n",
    "                    res_las = []\n",
    "                    for index_20m, row_20m in is2_laz.iterrows():\n",
    "                                \n",
    "                                lat_c = row_20m['land_segments/latitude_20m'] \n",
    "                                lon_c = row_20m['land_segments/longitude_20m'] \n",
    "                                slope = row_20m['slope']\n",
    "                                #print(out_wave, lat_c, lon_c, slope, '\\n')\n",
    "                                e1, n1 , zone1, letter1 = utm.from_latlon(lat_c, lon_c)\n",
    "                                theta = math.atan(slope)\n",
    "                                data = np.arange(-14, 15) # -14 --14\n",
    "                                e_array = e1 + data * 0.7*math.cos(theta)\n",
    "                                n_array = n1 + data * 0.7*math.sin(theta)\n",
    "                                #print('# e_array:', e_array.dtype)\n",
    "                                e_array = [f'{round(e, 6):.6f}' for e in e_array]\n",
    "                                n_array = [f'{round(e, 6):.6f}' for e in n_array]  \n",
    "                                # get unique footprint ID like string. \n",
    "                                # Concatenate elements with '.' separator\n",
    "                                is2_footprintID = [str(e) + '.' + str(n) for e, n in zip(e_array, n_array)]\n",
    "                                #print('# is2_footprintID', is2_footprintID)\n",
    "                                ##### I have utm now, return waveid  rowindex start and end. \n",
    "                                # get wave id start index\n",
    "                                df2 = pd.DataFrame(is2_footprintID)\n",
    "                                df2.columns = ['id']\n",
    "                                res = pd.merge(df1, df2, on='id', how='inner')\n",
    "                                #print('res:' , res)\n",
    "                                start = res['wave_index'].iloc[0]\n",
    "                                end = res['wave_index'].iloc[-1]\n",
    "                                #print('start, end: ', start, end)\n",
    "                                rh = get_sim_rh(out_wave, start, end)\n",
    "                                rh['fid'] = index_20m\n",
    "                                res_las.append(rh)\n",
    "                    f_wave.close()            \n",
    "                    res_las = pd.concat(res_las, ignore_index=True)\n",
    "                    out_rh = '../simResult/rh_'+ basename[:-4] + '.parquet'\n",
    "                    res_las.to_parquet(out_rh)\n",
    "                    #segment_footprints_utm[['e', 'n']].to_csv(out_coor, sep=' ', header = False,  index = False)\n",
    "                    #print('# get ch98 , rh and fid ???')\n",
    "                    # df_tmp = pd.DataFrame()\n",
    "                    # try:\n",
    "                    #      df_tmp = get_sim_rh(out_wave)\n",
    "                    #      df_tmp['fid'] = fid\n",
    "                    # #      print('# sim result length: ', len(df_tmp))\n",
    "                    #      results.append(df_tmp)\n",
    "                    # except:\n",
    "                    #      pass\n",
    "                    # if (len(results) > 0):\n",
    "                    #             # write to parquet \n",
    "                    #             las_sim_result = pd.concat(results, ignore_index=True)\n",
    "                    #             print('# write simulation result for this las file...')\n",
    "                    #             las_sim_result.to_parquet('../wave/sim_las_'+ basename[:-3] + 'parquet')\n",
    "                    #             #return pd.concat(results, ignore_index=True)\n",
    "#laz_path = '/gpfs/data1/vclgp/data/gedi/imported/usa/neon_sawb/LAZ_ground/L26_2014_BURL_1_v01_2014053013_P01_r.laz'                        \n",
    "#sim_is2_laz(is2_in_als_utm , laz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "feede308-3c17-45c8-805f-40f080e40ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_rh(filename, start, end):\n",
    "                    # Canopy height, ch\n",
    "                    # For a 100 m segment, the cumulative height distribution of canopy photons is \n",
    "                    # generated and the height at the 98th percentile.\n",
    "                    with h5py.File(filename, \"r\") as f:\n",
    "                        # Print all root level object names (aka keys) \n",
    "                        # these can be group or dataset names \n",
    "                        #print(\"Keys: %s\" % f.keys())\n",
    "                        N_foorprints = f['RXWAVECOUNT'].shape[0]\n",
    "                        pho_w_ground = pd.DataFrame()\n",
    "                        pho_no_ground = pd.DataFrame()\n",
    "                        for i in range(start, end+1): # inclu\n",
    "                                    \n",
    "                                    RXWAVECOUNT = f['RXWAVECOUNT'][i]\n",
    "                                    GRWAVECOUNT = f['GRWAVECOUNT'][i]\n",
    "                                    zStart = f[\"Z0\"][i]\n",
    "                                    zEnd = f[\"ZN\"][i]\n",
    "                                    zG = f[\"ZG\"][i]\n",
    "                                    wfCount = f[\"NBINS\"][0]\n",
    "                                    if (wfCount < 1): continue\n",
    "                                    wfStart = 1\n",
    "                                    # Calculate zStretch\n",
    "                                    zStretch = zEnd + (np.arange(wfCount, 0, -1) * ((zStart - zEnd) / wfCount))\n",
    "                                    #plt.plot(RXWAVECOUNT, zStretch,color='red' )\n",
    "                                    #plt.plot(RXWAVECOUNT - GRWAVECOUNT, zStretch,color='black', linestyle='--' )\n",
    "                                    #plt.show()\n",
    "                                    # sampling \n",
    "                                    n = np.random.poisson(3,1)[0] # posson sample, how many photons?\n",
    "                                    \n",
    "                                    # canopy + ground photons\n",
    "                                    rows = np.arange(RXWAVECOUNT.shape[0])\n",
    "                                    total = sum(RXWAVECOUNT)\n",
    "                                    \n",
    "                                    # Normalize the data by dividing each value by the total\n",
    "                                    p_data = [value / total for value in RXWAVECOUNT]\n",
    "                                    photon_rows = np.random.choice(rows, size=n, p=p_data)\n",
    "                                    df1 = zStretch[photon_rows] - zG\n",
    "                                    df1 = pd.DataFrame(df1)\n",
    "                                    if (len(pho_w_ground) == 0):\n",
    "                                        pho_w_ground = df1\n",
    "                                    else:\n",
    "                                        #print(df1)\n",
    "                                        pho_w_ground = pd.concat([pho_w_ground, df1], ignore_index=True)\n",
    "                                    # # canopy photons only\n",
    "                                    canopy_wave = RXWAVECOUNT - GRWAVECOUNT\n",
    "                                    \n",
    "                                        \n",
    "                                    total = sum(canopy_wave)\n",
    "                                    if (total == 0 ): continue\n",
    "                                    # fill NAs.\n",
    "                                    #canopy_wave = np.nan_to_num(canopy_wave, nan=0.0)\n",
    "                                    # Normalize the data by dividing each value by the total\n",
    "                                    p_data = [value / total for value in canopy_wave]\n",
    "                                    photon_rows = np.random.choice(rows, size=n, p=p_data)\n",
    "                                    df1 = zStretch[photon_rows] - zG\n",
    "                                    df1 = pd.DataFrame(df1)\n",
    "                                    if (len(pho_no_ground) == 0):\n",
    "                                        pho_no_ground = df1\n",
    "                                    else:\n",
    "                                        pho_no_ground = pd.concat([pho_no_ground, df1], ignore_index=True)\n",
    "                    f.close()\n",
    "                    percentiles = np.arange(1, 101, 1)\n",
    "                    height_percentiles = np.percentile(pho_no_ground, percentiles)\n",
    "                    ch_98 = height_percentiles[97] ### h_98\n",
    "                    \n",
    "                    # rh \n",
    "                    percentiles = np.arange(0, 101, 1)\n",
    "                    #0 --> min height\n",
    "                    # 100 --> max height\n",
    "                    height_percentiles = np.percentile(pho_w_ground, percentiles)\n",
    "                    # data frame from 0 to 100\n",
    "                    column_names = [\"rh\" + str(i) for i in range(101)]  # Creates a list from rh0 to rh100\n",
    "                    sim = pd.DataFrame(columns=column_names)\n",
    "                    # Add your array as a row to the DataFrame\n",
    "                    sim.loc[0] = height_percentiles\n",
    "                    sim['h_canopy_98'] = ch_98\n",
    "                    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "27b76a11-9c7b-46f0-8459-541af6fc0609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ab3ca918-5f31-4857-b1d9-e64099212bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh0</th>\n",
       "      <th>rh1</th>\n",
       "      <th>rh2</th>\n",
       "      <th>rh3</th>\n",
       "      <th>rh4</th>\n",
       "      <th>rh5</th>\n",
       "      <th>rh6</th>\n",
       "      <th>rh7</th>\n",
       "      <th>rh8</th>\n",
       "      <th>rh9</th>\n",
       "      <th>...</th>\n",
       "      <th>rh93</th>\n",
       "      <th>rh94</th>\n",
       "      <th>rh95</th>\n",
       "      <th>rh96</th>\n",
       "      <th>rh97</th>\n",
       "      <th>rh98</th>\n",
       "      <th>rh99</th>\n",
       "      <th>rh100</th>\n",
       "      <th>h_canopy_98</th>\n",
       "      <th>fid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.112846</td>\n",
       "      <td>-0.979255</td>\n",
       "      <td>-0.682002</td>\n",
       "      <td>-0.272380</td>\n",
       "      <td>0.074971</td>\n",
       "      <td>0.307222</td>\n",
       "      <td>0.470962</td>\n",
       "      <td>0.578526</td>\n",
       "      <td>0.597129</td>\n",
       "      <td>0.616675</td>\n",
       "      <td>...</td>\n",
       "      <td>26.776775</td>\n",
       "      <td>27.185872</td>\n",
       "      <td>27.510227</td>\n",
       "      <td>27.859304</td>\n",
       "      <td>28.354926</td>\n",
       "      <td>29.165505</td>\n",
       "      <td>30.486582</td>\n",
       "      <td>32.418333</td>\n",
       "      <td>27.934411</td>\n",
       "      <td>14822711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.196144</td>\n",
       "      <td>-4.954432</td>\n",
       "      <td>-4.655546</td>\n",
       "      <td>-3.657967</td>\n",
       "      <td>-1.966265</td>\n",
       "      <td>-1.270073</td>\n",
       "      <td>-1.161942</td>\n",
       "      <td>-1.153060</td>\n",
       "      <td>-1.081060</td>\n",
       "      <td>-0.853060</td>\n",
       "      <td>...</td>\n",
       "      <td>18.081778</td>\n",
       "      <td>18.116287</td>\n",
       "      <td>18.130810</td>\n",
       "      <td>18.164060</td>\n",
       "      <td>19.118424</td>\n",
       "      <td>20.005204</td>\n",
       "      <td>20.821157</td>\n",
       "      <td>21.919175</td>\n",
       "      <td>24.430088</td>\n",
       "      <td>14822712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.787182</td>\n",
       "      <td>-2.532182</td>\n",
       "      <td>-2.175873</td>\n",
       "      <td>-1.877455</td>\n",
       "      <td>-1.442455</td>\n",
       "      <td>-0.988518</td>\n",
       "      <td>-0.948283</td>\n",
       "      <td>-0.706667</td>\n",
       "      <td>-0.692455</td>\n",
       "      <td>-0.206374</td>\n",
       "      <td>...</td>\n",
       "      <td>15.708545</td>\n",
       "      <td>15.735792</td>\n",
       "      <td>15.796568</td>\n",
       "      <td>16.302861</td>\n",
       "      <td>16.729275</td>\n",
       "      <td>17.100764</td>\n",
       "      <td>17.822565</td>\n",
       "      <td>18.644054</td>\n",
       "      <td>18.187183</td>\n",
       "      <td>14823351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.865242</td>\n",
       "      <td>-2.724478</td>\n",
       "      <td>-2.480056</td>\n",
       "      <td>-2.382925</td>\n",
       "      <td>-2.150531</td>\n",
       "      <td>-2.075107</td>\n",
       "      <td>-2.054038</td>\n",
       "      <td>-1.111345</td>\n",
       "      <td>-1.005919</td>\n",
       "      <td>-0.841100</td>\n",
       "      <td>...</td>\n",
       "      <td>12.599616</td>\n",
       "      <td>12.851235</td>\n",
       "      <td>13.717954</td>\n",
       "      <td>14.070896</td>\n",
       "      <td>14.233718</td>\n",
       "      <td>15.570755</td>\n",
       "      <td>15.710014</td>\n",
       "      <td>17.449644</td>\n",
       "      <td>13.029416</td>\n",
       "      <td>14823972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.788988</td>\n",
       "      <td>-1.555515</td>\n",
       "      <td>-1.322043</td>\n",
       "      <td>-1.169638</td>\n",
       "      <td>-1.098300</td>\n",
       "      <td>-1.026963</td>\n",
       "      <td>-1.026963</td>\n",
       "      <td>-1.026963</td>\n",
       "      <td>-1.022104</td>\n",
       "      <td>-1.012388</td>\n",
       "      <td>...</td>\n",
       "      <td>14.610835</td>\n",
       "      <td>14.743130</td>\n",
       "      <td>14.875424</td>\n",
       "      <td>14.943471</td>\n",
       "      <td>15.011518</td>\n",
       "      <td>15.370693</td>\n",
       "      <td>16.020996</td>\n",
       "      <td>16.671299</td>\n",
       "      <td>14.203593</td>\n",
       "      <td>14824628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14825255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14822709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.481717</td>\n",
       "      <td>-1.592779</td>\n",
       "      <td>-1.404435</td>\n",
       "      <td>-1.323310</td>\n",
       "      <td>-0.988443</td>\n",
       "      <td>-0.838313</td>\n",
       "      <td>-0.824842</td>\n",
       "      <td>-0.787239</td>\n",
       "      <td>-0.753265</td>\n",
       "      <td>-0.627152</td>\n",
       "      <td>...</td>\n",
       "      <td>20.091528</td>\n",
       "      <td>20.429036</td>\n",
       "      <td>21.419010</td>\n",
       "      <td>21.988261</td>\n",
       "      <td>22.231408</td>\n",
       "      <td>22.336672</td>\n",
       "      <td>22.348553</td>\n",
       "      <td>24.999180</td>\n",
       "      <td>25.884180</td>\n",
       "      <td>14822710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rh0       rh1       rh2       rh3       rh4       rh5       rh6  \\\n",
       "0 -1.112846 -0.979255 -0.682002 -0.272380  0.074971  0.307222  0.470962   \n",
       "1 -5.196144 -4.954432 -4.655546 -3.657967 -1.966265 -1.270073 -1.161942   \n",
       "2 -2.787182 -2.532182 -2.175873 -1.877455 -1.442455 -0.988518 -0.948283   \n",
       "3 -2.865242 -2.724478 -2.480056 -2.382925 -2.150531 -2.075107 -2.054038   \n",
       "4 -1.788988 -1.555515 -1.322043 -1.169638 -1.098300 -1.026963 -1.026963   \n",
       "5       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "6       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "7 -2.481717 -1.592779 -1.404435 -1.323310 -0.988443 -0.838313 -0.824842   \n",
       "\n",
       "        rh7       rh8       rh9  ...       rh93       rh94       rh95  \\\n",
       "0  0.578526  0.597129  0.616675  ...  26.776775  27.185872  27.510227   \n",
       "1 -1.153060 -1.081060 -0.853060  ...  18.081778  18.116287  18.130810   \n",
       "2 -0.706667 -0.692455 -0.206374  ...  15.708545  15.735792  15.796568   \n",
       "3 -1.111345 -1.005919 -0.841100  ...  12.599616  12.851235  13.717954   \n",
       "4 -1.026963 -1.022104 -1.012388  ...  14.610835  14.743130  14.875424   \n",
       "5       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "6       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "7 -0.787239 -0.753265 -0.627152  ...  20.091528  20.429036  21.419010   \n",
       "\n",
       "        rh96       rh97       rh98       rh99      rh100  h_canopy_98  \\\n",
       "0  27.859304  28.354926  29.165505  30.486582  32.418333    27.934411   \n",
       "1  18.164060  19.118424  20.005204  20.821157  21.919175    24.430088   \n",
       "2  16.302861  16.729275  17.100764  17.822565  18.644054    18.187183   \n",
       "3  14.070896  14.233718  15.570755  15.710014  17.449644    13.029416   \n",
       "4  14.943471  15.011518  15.370693  16.020996  16.671299    14.203593   \n",
       "5        NaN        NaN        NaN        NaN        NaN          NaN   \n",
       "6        NaN        NaN        NaN        NaN        NaN          NaN   \n",
       "7  21.988261  22.231408  22.336672  22.348553  24.999180    25.884180   \n",
       "\n",
       "        fid  \n",
       "0  14822711  \n",
       "1  14822712  \n",
       "2  14823351  \n",
       "3  14823972  \n",
       "4  14824628  \n",
       "5  14825255  \n",
       "6  14822709  \n",
       "7  14822710  \n",
       "\n",
       "[8 rows x 103 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('/gpfs/data1/vclgp/xiongl/ProjectIS2CalVal/simResult/rh_Fernan_11.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3f0da-1225-4a18-b370-11a9ffcb8bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "90d9689f-d205-47cf-b1a2-d224d0a9b47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../wave/wave_or_22_137.h5'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_wave = '../wave/wave_'+ basename[:-4] + '.h5'\n",
    "# List of input HDF5 file paths (replace with your file paths)\n",
    "# no need to merge, process  laz file by laz file. \n",
    "data_sim = '../wave/wave_*.h5'\n",
    "files = glob.glob(data_sim)\n",
    "print(len(files))\n",
    "files[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0a6af776-91d2-4019-a6f3-632ecfacf5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file: ['BEAMDENSE', 'FSIGMA', 'GRWAVECOUNT', 'GRWAVEFRAC', 'GRWAVEINT', 'IDLENGTH', 'INCIDENTANGLE', 'LAT0', 'LON0', 'NBINS', 'NPBINS', 'NTYPEWAVES', 'NWAVES', 'POINTDENSE', 'PRES', 'PSIGMA', 'PULSE', 'RXWAVECOUNT', 'RXWAVEFRAC', 'RXWAVEINT', 'SLOPE', 'WAVEID', 'Z0', 'ZG', 'ZGDEM', 'ZN']\n"
     ]
    }
   ],
   "source": [
    "# read h5 file , merge all h5 files \n",
    "import h5py\n",
    "# Open the HDF5 file for reading\n",
    "# waveform file:  ../wave/wave_Fernan_11.h5\n",
    "file = h5py.File('../wave/wave_Fernan_54.h5', 'r')\n",
    "# List the keys (datasets and groups) in the HDF5 file\n",
    "print(\"Keys in HDF5 file:\", list(file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "78ae5ba4-051e-4612-9dd3-7db1de7b9d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_strings = file['WAVEID'][()]\n",
    "wave_ids = []\n",
    "for item in byte_strings:\n",
    "        # Convert each byte string to a regular string and join them\n",
    "        result_string = ''.join([byte.decode('utf-8') for byte in item])\n",
    "        wave_ids.append(result_string)\n",
    "# Find common strings\n",
    "df1 = pd.DataFrame(wave_ids)\n",
    "df1.columns = ['id']\n",
    "df1['wave_idex'] = df1.index\n",
    "df2 = pd.DataFrame(is2_footprintID)\n",
    "df2.columns = ['id']\n",
    "res = pd.merge(df1, df2, on='id', how='inner')\n",
    "start = res['wave_idex'].iloc[0]\n",
    "end = res['wave_idex'].iloc[-1]\n",
    "start.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f3ac31e4-f0d0-442c-961a-c0ce268c048d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['546550.996991.5291964.621877',\n",
       " '546551.068802.5291963.925570',\n",
       " '546551.140613.5291963.229264',\n",
       " '546551.212424.5291962.532957',\n",
       " '546551.284234.5291961.836650',\n",
       " '546551.356045.5291961.140343',\n",
       " '546551.427856.5291960.444036',\n",
       " '546551.499666.5291959.747729',\n",
       " '546551.571477.5291959.051422',\n",
       " '546551.643288.5291958.355116',\n",
       " '546551.715099.5291957.658809',\n",
       " '546551.786909.5291956.962502',\n",
       " '546551.858720.5291956.266195',\n",
       " '546551.930531.5291955.569888',\n",
       " '546552.002342.5291954.873581',\n",
       " '546552.074152.5291954.177275',\n",
       " '546552.145963.5291953.480968',\n",
       " '546552.217774.5291952.784661',\n",
       " '546552.289585.5291952.088354',\n",
       " '546552.361395.5291951.392047',\n",
       " '546552.433206.5291950.695740',\n",
       " '546552.505017.5291949.999434',\n",
       " '546552.576828.5291949.303127',\n",
       " '546552.648638.5291948.606820',\n",
       " '546552.720449.5291947.910513',\n",
       " '546552.792260.5291947.214206',\n",
       " '546552.864070.5291946.517899',\n",
       " '546552.935881.5291945.821592',\n",
       " '546553.007692.5291945.125286']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# waveform file:  ../wave/wave_Fernan_11.h5\n",
    "#../wave/wave_Fernan_54.h5  47.779273986816406   -116.37858581542969   -9.696416800718838 \n",
    "lat_c = 47.779273986816406 \n",
    "lon_c = -116.37858581542969\n",
    "slope = -9.696416800718838 \n",
    "e1, n1 , zone1, letter1 = utm.from_latlon(lat_c, lon_c)\n",
    "theta = math.atan(slope)\n",
    "data = np.arange(-14, 15) # -14 --14\n",
    "e_array = e1 + data * 0.7*math.cos(theta)\n",
    "n_array = n1 + data * 0.7*math.sin(theta)\n",
    "\n",
    "e_array = [f'{round(e, 6):.6f}' for e in e_array]\n",
    "n_array = [f'{round(e, 6):.6f}' for e in n_array]\n",
    "\n",
    "is2_footprintID = [str(e) + '.' + str(n) for e, n in zip(e_array, n_array)]\n",
    "is2_footprintID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff52d0eb-dfd1-4d96-a915-ec020b405dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf1 = gpd.read_parquet(\"/gpfs/data1/vclgp/xiongl/ProjectIS2CalVal/data/all_sites_20231218.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87bb9428-249e-449a-bccb-19bbc87921a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>name</th>\n",
       "      <th>area_ha</th>\n",
       "      <th>epsg</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usa</td>\n",
       "      <td>usda_me</td>\n",
       "      <td>562710.549007</td>\n",
       "      <td>26919</td>\n",
       "      <td>MULTIPOLYGON (((-70.48145 45.36351, -70.48145 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usa</td>\n",
       "      <td>plumasnf_20180707</td>\n",
       "      <td>861751.014503</td>\n",
       "      <td>3310</td>\n",
       "      <td>MULTIPOLYGON (((-121.56314 39.67990, -121.5631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>australia</td>\n",
       "      <td>tern_robson_whole</td>\n",
       "      <td>3428.721642</td>\n",
       "      <td>28355</td>\n",
       "      <td>POLYGON ((145.62333 -17.12994, 145.61720 -17.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seasia</td>\n",
       "      <td>jpl_borneo_013</td>\n",
       "      <td>3829.727945</td>\n",
       "      <td>32750</td>\n",
       "      <td>POLYGON ((114.46872 -2.15843, 114.46872 -2.158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usa</td>\n",
       "      <td>stri_serc_Oct2011</td>\n",
       "      <td>1263.062788</td>\n",
       "      <td>32618</td>\n",
       "      <td>MULTIPOLYGON (((-76.56098 38.87436, -76.56098 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      region               name        area_ha   epsg  \\\n",
       "0        usa            usda_me  562710.549007  26919   \n",
       "1        usa  plumasnf_20180707  861751.014503   3310   \n",
       "2  australia  tern_robson_whole    3428.721642  28355   \n",
       "3     seasia     jpl_borneo_013    3829.727945  32750   \n",
       "4        usa  stri_serc_Oct2011    1263.062788  32618   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-70.48145 45.36351, -70.48145 ...  \n",
       "1  MULTIPOLYGON (((-121.56314 39.67990, -121.5631...  \n",
       "2  POLYGON ((145.62333 -17.12994, 145.61720 -17.1...  \n",
       "3  POLYGON ((114.46872 -2.15843, 114.46872 -2.158...  \n",
       "4  MULTIPOLYGON (((-76.56098 38.87436, -76.56098 ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e3047-a4f4-46ea-9f14-4b13a9073475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
